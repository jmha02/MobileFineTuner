# GPT-2 Full Finetune

- Main program: `main.cpp`
- Pretrained: `pretrained/gpt2/` (includes `config.json, merges.txt, tokenizer.json, vocab.json, model.safetensors`)

Usage (example):
- CMake target generated by `operators` project: `gpt2_full_finetune`
- Default path points to `pretrained/gpt2` under this directory
- Example execution:
  - `./gpt2_full_finetune --data_dir /path/to/wikitext-2-raw --pretrained_dir ./pretrained/gpt2 --output_path ./checkpoints/gpt2_full.safetensors --epochs 1 --batch_size 2 --grad_accum_steps 4 --seq_len 128`

